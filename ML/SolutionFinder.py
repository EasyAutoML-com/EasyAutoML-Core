from typing import Union, Optional, NoReturn
import numpy as np
from scipy.optimize import differential_evolution
from functools import lru_cache
import pandas as pd
from timeit import default_timer

from ML import EasyAutoMLDBModels

from ML import SolutionScore, Experimenter

from SharedConstants import *


from ML import __getlogger

logger = __getlogger()


# we cannot return none when the evaluation score fail, so we return this very low score
DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE = -999999


class SolutionFinder:
    """
    Find the input giving the best score for the given experience_inputs
    usually the outputs are only for evaluating the score

    the experience_inputs given are a list of all possible values for all experience_inputs and it is given by argument SolutionScore
    SolutionFinder will try any combination of possible values, then will pass to experimenter to generate the outputs, then SolutionScore will be used to evaluate the score of the combination

    :usage: self.find_solution( ) is the main method

    """

    def __init__(self, solution_finder_name: str):
        self.solution_finder_name = solution_finder_name
        self.result_shorter_cycles_enabled = None
        self.result_best_solution_final_score = None
        self.result_evaluate_count_run = None
        self.result_evaluate_count_better_score = None
        self.result_evaluate_count_no_score = None
        self.result_dict_solution_found_best_values = None
        self.result_delay_sec = None

    # todo move argument in the init, rename this function run
    def find_solution(
            self,
            possible_values_constant: dict,
            possible_values_varying: dict,
            solution_score: SolutionScore,
            experimenter: Experimenter,
    ):
        """
        Find the outputs giving the best score for the given experience_inputs
        possible_values contains all possibles values , it contains fixed and list of possible values for every columns
        Experimenter will generate the outputs from the experience_inputs combination
        SolutionScore contains a method to evaluate the score of a experience_inputs+outputs row

        :param possible_values_constant: contains all values which not vary but are necessary to describe the context
        :param possible_values_varying: it contains all possible values of experience_inputs, so we will try to combine any possible values to generate a ROW to test.
        :param solution_score: we want to find the best outputs, so we need to score the outputs found, we also do this with SolutionScore
        :param experimenter: we need to generate the outputs from the experience_inputs, so we use an Experimenter
        :return: a dataframe with one single row which is a combination of any possible values of every columns, plus the outputs generated by the experimenter which was giving the best score
        """
        if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"SF {self.solution_finder_name} Starting find_solution...." )

        self.result_shorter_cycles_enabled = None
        self.result_best_solution_final_score = None
        self.result_evaluate_count_run = None
        self.result_evaluate_count_better_score = None
        self.result_evaluate_count_no_score = None
        self.result_dict_solution_found_best_values = None
        self.result_delay_sec = None

        # differential_evolution do not return the best solution when maxiteration stop it - so we store the best solution
        best_solution_values_index = None
        best_solution_final_score = DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE
        evaluate_count_run = 0
        evaluate_count_no_score = 0
        evaluate_count_better_score = 0

        start_timer_sec = default_timer( )

        # ----_differential_evolution_evaluate_list_int--------------------------------------------------------------------------------------------------------------------------
        @lru_cache(maxsize=None)
        def _differential_evolution_evaluate_list_int( possible_values_varying_solution_tuple_index_int, ):
            """
            differential_evolution build a row with a combination of values and call this function to evaluate the score

            to evaluate the score we use nn_easyautoml_solution_finder , it try to predict the outputs or use the experimenter if prediction is not available
            then solution_score evaluate the score of the input+outputs

            differential_evolution make the index varying as float, we need to round them to get the values they index, but then when values change slowly the round(index) do not vary
            so to make faster we call function after rounding the index, and we cache the result of the function

            :return: we return an integer with the high value is best, if function fail to evaluate it return DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE

            """

            # this are variable used here from upper function that we update in this nested function
            nonlocal best_solution_final_score
            nonlocal best_solution_values_index

            # this are variable used here from upper function that we read in this nested function
            nonlocal possible_values_constant
            nonlocal possible_values_varying

            nonlocal evaluate_count_run
            nonlocal evaluate_count_no_score
            nonlocal evaluate_count_better_score

            evaluate_count_run += 1
            # we need to convert the index possible_values_varying_solution_index in real data values with lookup
            try:
                dict_possible_values_varying_solution_values = {
                    col_name : possible_values_varying[col_name ][ round( possible_values_varying_solution_tuple_index_int[ column_index ] ) ]
                    for column_index, col_name in enumerate( possible_values_varying.keys() )
                }

                all_inputs_values_to_experiment_or_predict = {}
                all_inputs_values_to_experiment_or_predict.update( possible_values_constant )
                all_inputs_values_to_experiment_or_predict.update( dict_possible_values_varying_solution_values )
            except Exception as e:
               logger.error( f"Initialization values of _differential_evolution_evaluate failed because {e} " )

            try:
                # we predict (or do experimenter) the experience_inputs to get the outputs
                machine_easyautoml_solution_finder_predictions = (machine_easyautoml_solution_finder.do_predict( all_inputs_values_to_experiment_or_predict ))
            except Exception as e:
                logger.warning(  f"Unable to run machine_easyautoml_solution_finder  inside _differential_evolution_evaluate because error: {e}" )
                machine_easyautoml_solution_finder_predictions = None

            if machine_easyautoml_solution_finder_predictions is None:
                if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"_differential_evolution_evaluate : No prediction valid from {machine_easyautoml_solution_finder}" )
                evaluate_count_no_score += 1
                return DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE

            # we evaluate the score of the solution:experience_inputs+outputs
            try:
                score = solution_score.eval( machine_easyautoml_solution_finder_predictions )[ 0 ]
            except Exception as e:
                logger.error( f"Unable to evaluate the score of the solution solution_score in _differential_evolution_evaluate ! " )

            if not score:
                if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"_differential_evolution_evaluate have no score (failed)" )
                evaluate_count_no_score += 1
                return DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE


            if score > best_solution_final_score:
                # store best solution because differential_evolution when stop by maxiteration do not return the best solution
                best_solution_values_index = possible_values_varying_solution_tuple_index_int
                best_solution_final_score = score
                evaluate_count_better_score += 1
                if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"Evaluate_score: {score:.2f} - NEWBEST - for prediction : {[ str(k ) + ':' + str(round(v[0],2) ) for k,v in machine_easyautoml_solution_finder_predictions.to_dict( ).items( ) ]} " )
            else:
                if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"Evaluate_score: {score:.2f} -                   - for prediction : {[ str( k ) + ':' + str( round( v[ 0 ], 2 ) ) for k, v in machine_easyautoml_solution_finder_predictions.to_dict( ).items( ) ]} " )

            return score
            # ----_differential_evolution_evaluate_list_int---------------------------------------------------------------------------------


        # ----_differential_evolution_evaluate_index_float----------------------------------------------------------------------------------
        def _differential_evolution_evaluate_index_float( possible_values_varying_solution_index_float: np.ndarray, ):
            # we round the indexes and call the function having LRU cache
            #try:
            result = _differential_evolution_evaluate_list_int( tuple( np.around( possible_values_varying_solution_index_float ).tolist()) )
            return result
            #except Exception as e:
            #    logger.error( f"Error {e} in _differential_evolution_evaluate_list_int. (possible_values_varying_solution_index_float={possible_values_varying_solution_index_float})" )
        # -----_differential_evolution_evaluate_index_float-----------------------------------------------------------------------------


        # Function to find the nearest index in the list of possible values
        def find_nearest_index( value, possible_values )->int:
            if isinstance(value, str):
                try:
                    for i, item in enumerate(possible_values):
                        if item.lower().strip() == value.lower().strip():
                            return i
                except Exception as e:
                    pass
                logger.error( f"Unable to find this string value '{value}' in list of possible values : {possible_values} " )
            else:
                try:
                    nearest_value = None
                    nearest_index = None
                    min_difference = float('inf')  # Initialize with positive infinity
                    for i, possible_val in enumerate(possible_values):
                        difference = abs(possible_val - value)  # Calculate absolute difference
                        if difference < min_difference:
                            min_difference = difference
                            nearest_value = possible_val
                            nearest_index = i
                    return nearest_index
                except Exception as e:
                    pass
                logger.error( f"Unable to find nearest numeric value '{value}' in list of possible values : {possible_values} " )


        # ----------------------------------------------------------------------------------------------------------------------------------
        # find_solution
        # ----------------------------------------------------------------------------------------------------------------------------------
        if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug(f"Starting: ({self.solution_finder_name}) ")

        # MachineEasyAutoML is used to predict the outputs , it will use the Experimenter if the prediction are not available
        # We have one MachineEasyAutoML for every find_solution because every find_solution is design for a different problem and work anyway with a different structure of experience_inputs-outputs
        from ML import MachineEasyAutoML
        machine_easyautoml_solution_finder = MachineEasyAutoML(
            f"__SF_Experiences_{self.solution_finder_name}__",
            optional_experimenter=experimenter,
        )
        machine_easyautoml_solution_finder_is_trained_and_ready = machine_easyautoml_solution_finder._machine and machine_easyautoml_solution_finder._machine.is_nn_solving_ready( )
        self.result_shorter_cycles_enabled = DEBUG_FORCE_FASTER_SHORTER_DIFFERENTIAL_EVOLUTION or not machine_easyautoml_solution_finder_is_trained_and_ready


        # Differential evolution work with index not with real experience_inputs values
        # so it need the bound (how many possible values there is in for each columns in dataset_user_input_possible_values)
        possible_values_varying_bounds_min_max = [
            (0, len(col_list_values) - 1)
            for col_list_values in possible_values_varying.values()
        ]

        # if possible, predict the solution directly, this will be used as base for searching solution (x0)
        sf_initial_x0_prediction = None
        nni=MachineEasyAutoML( f"__SF_Results_{self.solution_finder_name}__" )
        if nni.ready_to_predict():
            outputs_predicted_df = nni.do_predict( possible_values_constant )
            if outputs_predicted_df is not None:
                # Verify that outputs_predicted has all keys of possible_values_varying
                missing_keys = [key for key in possible_values_varying.keys() if key not in outputs_predicted_df.columns]
                if missing_keys:
                    logger.warning( f"The prediction of sf_initial_x0_prediction have this keys missing : {missing_keys}" )
                # Verify that outputs_predicted does not have additional keys than possible_values_varying - note that key starting with result_ are normal , they are just extra predictions we do not need here
                additional_keys = [key for key in outputs_predicted_df.columns if key not in possible_values_varying.keys() and not key.startswith( "result_") ]
                if additional_keys:
                    logger.warning(f"The prediction of sf_initial_x0_prediction have Additional keys in outputs_predicted: {additional_keys}")

                if not missing_keys and not additional_keys:
                    # Convert values predicted (outputs_predicted) in Index of Values inside (possible_values_varying)
                    sf_initial_x0_prediction = []
                    for key in possible_values_varying.keys():
                        predicted_value = outputs_predicted_df[ key ].iloc[0]
                        predicted_value_nearest_indice = find_nearest_index( predicted_value, possible_values_varying[ key ] )
                        sf_initial_x0_prediction.append( predicted_value_nearest_indice )

        # we run differential_evolution in args we put all data we will need to evaluate the possible solutions
        differential_evolution_param_polish = False if self.result_shorter_cycles_enabled  else True
        differential_evolution_param_maxiter = 4 if self.result_shorter_cycles_enabled  else 12
        differential_evolution_param_popsize = 2 if self.result_shorter_cycles_enabled else 4
        differential_evolution_param_tol = 0.05
        differential_evolution_result = differential_evolution(
                _differential_evolution_evaluate_index_float,
                possible_values_varying_bounds_min_max,
                integrality = [ True for i in range( 0 , len(possible_values_varying_bounds_min_max)) ] ,
                polish = differential_evolution_param_polish,
                updating = 'immediate',
                x0=sf_initial_x0_prediction,
                # Total Cycles = maxiter * popsize * len( possible_values_varying_bounds_min_max )
                maxiter = differential_evolution_param_maxiter ,  # The maximum number of generations over which the entire population is evolved. The maximum number of function evaluations (with no polishing) is: (maxiter + 1) * popsize * (N - N_equal)
                popsize = differential_evolution_param_popsize, # A multiplier for setting the total population size. The population has popsize * (N - N_equal) individuals.
                tol = differential_evolution_param_tol,  # pourcentage minimum d'amelioration sinon arret (Default=0.01)
        )

        # cache is cleared for _differential_evolution_evaluate_index_int
        if ENABLE_LOGGER_DEBUG_SolutionFinder: logger.debug( f"lru_cache: { _differential_evolution_evaluate_list_int.cache_info( ) }" )
        _differential_evolution_evaluate_list_int.cache_clear( )

        # we flush the buffer for this solution_finder
        machine_easyautoml_solution_finder.save_data()

        if best_solution_final_score == DIFFERENTIAL_EVOLUTION_FAIL_LOW_SCORE:
            logger.warning(f"solution_finder ({self.solution_finder_name}) have not find one single valid solution !")
            return None

        # score returned is not used : differential_evolution_result['fun']  because we use best_solution_final_score instead
        #dict_solution_found_best_index = differential_evolution_result["x"]
        # we prefer to use the stored best solution found rather than the latest one
        dict_solution_found_best_index = best_solution_values_index

        # we need to convert the index possible_values_varying_solution_index in real data values with lookup
        dict_solution_found_best_values = {
            col_name : possible_values_varying[col_name ][ round( dict_solution_found_best_index[ column_index ] ) ]
            for column_index, col_name in enumerate( possible_values_varying.keys() )
        }
        self.result_best_solution_final_score = best_solution_final_score
        self.result_evaluate_count_run = evaluate_count_run
        self.result_evaluate_count_better_score = evaluate_count_better_score
        self.result_evaluate_count_no_score = evaluate_count_no_score
        self.result_dict_solution_found_best_values = dict_solution_found_best_values
        self.result_delay_sec = default_timer( ) - start_timer_sec

        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug( f"-------------------------------------------------------------------------------------------")
        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug(f"SolutionFinder '{self.solution_finder_name}' done with shorter_cycles_enabled={self.result_shorter_cycles_enabled} with { self.result_evaluate_count_run } evaluate() in a delay of: { self.result_delay_sec } sec." )
        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug( f" Final Score: { self.result_best_solution_final_score } - We got better score { self.result_evaluate_count_better_score } times, and no_score { self.result_evaluate_count_no_score } times.  ")
        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug( f"-------------------------------------------------------------------------------------------")
        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug( dict_solution_found_best_values )
        if ENABLE_LOGGER_DEBUG_SolutionFinder_RESULT: logger.debug( f"-------------------------------------------------------------------------------------------")

        # record the best result for predicting directly the solutions (for X0)
        from ML import MachineEasyAutoML
        MachineEasyAutoML_inputs = possible_values_constant.copy()  # use copy or we change the argument of the main function

        MachineEasyAutoML_inputs.update( {
                'differential_evolution_param_polish' : differential_evolution_param_polish,
                'differential_evolution_param_maxiter' : differential_evolution_param_maxiter,
                'differential_evolution_param_popsize' : differential_evolution_param_popsize,
                'differential_evolution_param_tol' : differential_evolution_param_tol,
        } )
        MachineEasyAutoML_outputs = {
        "result_shorter_cycles_enabled" : self.result_shorter_cycles_enabled,
        "result_best_solution_final_score" : self.result_best_solution_final_score,
        "result_evaluate_count_run" : self.result_evaluate_count_run,
        "result_evaluate_count_better_score" : self.result_evaluate_count_better_score,
        "result_evaluate_count_no_score" : self.result_evaluate_count_no_score,
        "result_delay_sec" : self.result_delay_sec,
        }
        MachineEasyAutoML_outputs.update( dict_solution_found_best_values )
        MachineEasyAutoML( f"__SF_Results_{self.solution_finder_name}__" ).learn_this_inputs_outputs(
            inputsOnly_or_Both_inputsOutputs=MachineEasyAutoML_inputs,
            outputs_optional=MachineEasyAutoML_outputs,
        )

        # return the varying value with the best score found
        return dict_solution_found_best_values

